# -*- coding: utf-8 -*-
"""New Project Sentiment_Analysis_.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N12WVylZtaiRUe97rhiKicyZQz0Ovevz
"""

!pip install spacy--upgrade
#pip install spacy==version
!python -m spacy download pt
!python install nltk --upgrade
!python3 -m spacy download pt

import spacy
from nltk.stem import PorterStemmer
from spacy import displacy
from spacy.lang.pt.examples import sentences
from spacy.lang.en.stop_words import STOP_WORDS
from spacy.lang.pt.stop_words import STOP_WORDSPT
import panda as pd
import string
import random
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
import re
from sklearn.metrics import accuracy_score, confusion_matrix
spacy.__version__, nltk.__version__

dataset_train = pd.read_csv('dataset_train.csv', encodig = 'utf-8')
dataset_train.shape
dataset_train.head()

dataset_train.tail()

sns.countplot(dataset_train['sentiment'], label = 'Count')

dataset_train.drop(['id','tweet_date', 'query_used'], axis=1, inplace=True)

dataset_train.head()

sns.heatmap(pd.isnull(dataset_train))

dataset_test = pd.read_csv('dataset_test.csv', encodig = 'utf-8')

dataset_test.shape
dataset_test.head()

dataset_test.tail()

sns.countplot(dataset_test['sentiment'], label = 'Count')

dataset_test.drop(['id','tweet_date', 'query_used'], axis=1, inplace=True)

dataset_test.head()

sns.heatmap(pd.isnull(dataset_test))

nlp = spacy.load('pt')
nlp

dataset_train['tweet_text'][1]

stop_words = spacy.lang.pt.stop_words.STOP_WORDS
print(stop_words)

string.punctuation

def preprocessing(text):
  #lowercase letters
  text = text.lowercase()

  #User Name
  text = re.sub(r"@[A-Za-z0-9$-_@.&+]+", ' ', text)

  #URL
  text = re.sub(r"https?://[A-Za-z0-9./]+", " ", text)

  #whitespace
  text = re.sub(r" +", " ", text)

  #Emotions
  list_emotions = {':)': 'positiveemotion',
                   ':d': 'positiveemotion',
                   ':(': 'negativeemotion'}

  for emotion in list_emotions:
    text  = text.replace(emotion, list_emotions[emotion])

    #Lemmatization
    doc = nlp(text)
    list_ = []
    for token in doc:
      list_.append(token.lemma_)

    #stop_words
    list_ = [word for word in list_ if word not in stop_words and word not in string.punctuation]
    list_ = ' '.join([str(elemento) for elemento in list_ if not elemento.isdigit()])

  return list_

text_test = dataset_train['tweet_text'][1]
result = preprocessing(text_test)
result

dataset_train.head(10)

dataset_train['tweet_text'] = dataset_train['tweet_text'].apply(preprocessing)

dataset_train.head(10)

dataset_test['tweet_text'] = dataset_test['tweet_text'].apply(preprocessing)

dataset_train_final = []
for text, emotion in zip(dataset_train['tweet_text'], dataset_train['sentiment']):
  if emotion == 1:
    dic = ({'Positive': True, "Negative": False})
  elif emotion == 0:
    dic = ({'Positive': False, 'Negative': True})

  dataset_train_final.append([text, dic])

len(dataset_train_final)
dataset_train_final[30000:30005]

model = spacy.blank('pt')
cat = model.create_pipe('textcat')
cat.add_label('Positive')
cat.add_label('Negative')
model.add_pipe(cat)
historical = []

model.begin_training()
for epochs in range(1000):
  random.shuffle(dataset_final)
  losses = {}
  for batch in spacy.utils.minibatch(dataset_final, 30):
    texts = [model(text) for text , entities in batch]
    annotations = [{'cats': entities} for text, entities in batch]
    model.update(texts, annotations, losses=losses)
  if epochs % 100 == 0:
    print(losses)
    historical.append(losses)

istorical_loss = []
for i in historical
  historical_loss.append(i.get('textcat'))

historical_loss = np.array(historical_loss)
historical_loss

plt.plot(historical_loss)
plt.title("Error progression")
plt.xlabel("Epochs")
plt.ylabel("Error")

model.to_disk('model')

model_load = spacy.load('model')
model_load

text_positive = dataset_test['tweet_text'][21]
text_positive

prev = model_load(text_positive)
prev

prev.cats

text_positive = 'eu gosto muito de vocÃª e 3313'
text_positive = preprocessing(text_positive)
text_positive

model_load(text_positive).cats

text_negative = dataset_test['tweet_text'][4000]
text_negative

prev = model_load(text_negative)
prev.cats