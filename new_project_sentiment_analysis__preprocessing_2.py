# -*- coding: utf-8 -*-
"""New Project Sentiment_Analysis _Preprocessing_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EyvrgCk64fEDaYmtN--xwOkdFY4Zc1hm
"""

!pip install spacy--upgrade
#pip install spacy==version
!python -m spacy download pt
!python install nltk --upgrade
!python3 -m spacy download pt

import spacy
from nltk.stem import PorterStemmer
from spacy import displacy
from spacy.lang.pt.examples import sentences
from spacy.lang.en.stop_words import STOP_WORDS
from spacy.lang.pt.stop_words import STOP_WORDSPT
import panda as pd
import string
import random
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
import re
from sklearn.metrics import accuracy_score, confusion_matrix
spacy.__version__, nltk.__version__

dataset_train = pd.read_csv('dataset_train.csv', encodig = 'utf-8')
dataset_train.shape
dataset_train.head()

dataset_train.tail()

sns.countplot(dataset_train['sentiment'], label = 'Count')

dataset_train.drop(['id','tweet_date', 'query_used'], axis=1, inplace=True)

dataset_train.head()

sns.heatmap(pd.isnull(dataset_train))

dataset_test = pd.read_csv('dataset_test.csv', encodig = 'utf-8')

dataset_test.shape
dataset_test.head()

dataset_test.tail()

sns.countplot(dataset_test['sentiment'], label = 'Count')

dataset_test.drop(['id','tweet_date', 'query_used'], axis=1, inplace=True)

dataset_test.head()

sns.heatmap(pd.isnull(dataset_test))

nlp = spacy.load('pt')
nlp

dataset_train['tweet_text'][1]

stop_words = spacy.lang.pt.stop_words.STOP_WORDS
print(stop_words)

string.punctuation

def preprocessing(text):
  #lowercase letters
  text = text.lowercase()

  #User Name
  text = re.sub(r"@[A-Za-z0-9$-_@.&+]+", ' ', text)

  #URL
  text = re.sub(r"https?://[A-Za-z0-9./]+", " ", text)

  #whitespace
  text = re.sub(r" +", " ", text)

  #Emotions
  list_emotions = {':)': 'positiveemotion',
                   ':d': 'positiveemotion',
                   ':(': 'negativeemotion'}

  for emotion in list_emotions:
    text  = text.replace(emotion, list_emotions[emotion])

    #Lemmatization
    doc = nlp(text)
    list_ = []
    for token in doc:
      list_.append(token.lemma_)

    #stop_words
    list_ = [word for word in list_ if word not in stop_words and word not in string.punctuation]
    list_ = ' '.join([str(elemento) for elemento in list_ if not elemento.isdigit()])

  return list_

text_test = dataset_train['tweet_text'][1]
result = preprocessing(text_test)
result